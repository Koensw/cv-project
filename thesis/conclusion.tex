%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------

\chapter{Conclusions and Future Work}
\label{ch:conclusion}
In this work the applicability of self-supervised learning using an ordening task on camera and lidar data was analyzed with the autonomous driving dataset Kitti\cite{geiger2012}. The primary goal was twofold: (1) investigating the performance of a self-supervised task on an autonomous driving dataset and (2) researching the strength of additional input features in the form of lidar data in various representations and examining if those could enhance performance when combined with camera data.

It has been shown that the network are definitely capable of learning to order frames, the binary ordering task has even been shown to be on the easy side for a convolutional neural network to learn. However even when using the more difficult 24-order permutation task, top-1 accuracies close to 60\% were reached, signalling strong and robust performance. The grounds for this strong performance on the self-supervised task were however hard to find. It was found that the quality of the network filters remain on a relatively simple scale, and only a limited number of filters are learned. Also more detailed investigations seem to suggest that the network does not learn details about particular objects and instead remain reliant on general correlation between images. To verify this behavior and to work on alternative approaches to make it harder to learn low-level features is an interesting direction for future work. Especially sampling different parts of the frame might be an worthwhile approach to investigate, although that would also imply getting rid of certain features in the field-of-view of the cameras making it impossible to use the frame consistency that is specifically apparent in driving datasets.  

From the point of view of using lidar to enhance the learning, it is directly clear that it provides strong features for learning. In particular the interpolated lidar depth has been a strong feature in this ordening task, much stronger than the unprocessed lidar. Also the lidar height has shown to be useful for learning, in contrast to the reflectances which did not contain sufficient information for the network to converge. More surprisingly, combinations of multiple input features did not shown any particular improvement, compared to the individual features. However this might come from the lack of input data as earlier fusion was much stronger, suggesting that the network has problems learning all the parameters. In this respect it might be interesting to consider training on larger datasets in the future.

The most important step for future work would be to test the investigated backbones on various alternative tasks like classification and object detection, to gain from the structure learnt by the self-supervised task, without direct supervision. Before this step is deemed viable, more research would be needed to improve the quality of the pre-training. If improvements are possible, the self-supervised task could possible also be used directly as input to learn driving models, which appears to be an interesting direction for future research. 

%Due to time constraints this step could unfortunately not yet been implemented as part of this thesis.

%does unfortuantely however not appear to be directly scalable to different problems in its current state.
