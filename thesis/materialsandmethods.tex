%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Approach and Implementation}
\label{ch:implementationdetails}
The main goal of this paper is to investigate the possiblities for self-supervised techniques on autonomous driving videos using CNN's. The focus is on an self-supervised ordering task exploiting spatiotemporal signals, using a similar approach as in previous works on using temporal coherence~\cite{misra2016,lee2017}, but then applied to videos captured by cars. Furthermore we add the usage of lidar data with the aim of learning richer representations. Different versions of the task are investigated as explained in more detail in the  Chapter \ref{ch:experimentsandresults}, but the basic idea of the task is as follows:
\begin{enumerate}    
\item A sequence of images is selected with sufficient motion
\item The sequence is split into separate samples and fed individually through a siamese backbone network with the goal of learning generic image features.
\item The output features of all images in the sequence of are subsequently used as input to an ordering network.
\item The ordering network predicts either if the ordering is correct (easy)\cite{misra2016} or the actual permutation (harder)\cite{lee2017}.
\end{enumerate}
An example of such a task is in\needfig.

\section{Generating train and test data}
As the task is unsupervised there is no need for labeled data, only a large number of video samples with precise lidar measurement. For this task one suitable choice is the Kitti\cite{geiger2012} visual odometry dataset, which is primarily used in all experiments. This dataset contains a total of 22 sequences covering a total length of 39.2 km of outdoor streets divided in over 41k frames sampled at 10 Hz. It contains high-quality images from two grayscale and two color PointGrey Flea2 video cameras with 1392Ã—512 pixel resolutions and corresponding lidar range scans from a Velodyne HDL-64E 3D laser scanner.

The dataset was split into 15 sequences for generating the training data and 7 independent sequences for testing. For generating the image data the RGB camera 02 was selected, the data from the other camera's was not used. Because the input images are of large dimensions for common backbone networks the images were first downscaled with anti-aliasing to a size of 227x227 as used in Caffenet~\cite{jia2014}. This resizing does not preserve scaling, but this is not a primary issue as all images are downsized in the same way and the structure of the objects remain clear in those smaller images. Finally for all experiments with single channel images channel splitting is employed, by randomly selecting one of the three RGB channels, it has been shown that this makes the data more rigid against learning low-level features then direct conversion to grayscale~\cite{lee2017}. The final images are saved as grayscale images and therefore these channel split images are simply referred grayscale images afterwards. In other experiments the data is used in full color.

Properly preparing training data with sufficiently balanced difficulty is of primary importance for proper learning in this task. Static sequences that are almost impossible to order should be prevented, but the network should also avoid learning low-level cues without semantic understanding. To generate the training and test data, sets of either 3 or 4 images both with uniform and non-uniform spacing within a certain timeframe are randomly sampled from the corresponding sequences (sequences are not all of same length, and sequences with more images are sampled more often). Then the magnitude of optical flow is calculated between the frames and only sequences with high magnitude are selected\cite{misra2016}, or more specifically where the mean intensity of the 10 highest optical flow regions is higher than a manually tuned value. 

%[It has also been suggested to mine smaller patches from the data\cite{lee2017}.]

The 3D lidar range data is projected to the location of camera 02, which was used to generate the image data. Auxilliary information in the form of reflectance values is not used in this study. Then the lidar data is reduced to 227x227 similar to the image data. The data at this stage is the unprocessed lidar data. [As the lidar data is sparse we also employ the blurring to help the network]

To generate a proper distribution of positive and negative samples, every image sequence is used twice for the binary ordering task with one positive (sorted) and negative (unsorted) sample added to both the training and test set. For the binary task both ascendingly sorted and inversely sorted are counted as a positive (sorted) sample following Misra et al., who are reasoning that for certain training instances it is hard to distinguish forward and backward sorting (for example picking up a coffee or placing it down), however for the permutation task all permutations are treated separately as we actually expect this to be often identifiable in autonomous driving datasets as the cars are almost always moving in the forward direction. For the permutation estimation network, four different permutations are used for every sampled sequence. As 10,000 sequences are sampled for the training set this results in respectively 20,000 training samples and 40,000 training samples for the two variations. Similary 10,000 and 20,000 image are generated for the different test sets. 

\section{Implementation in the Caffe framework}
A convolutional neural network approach was adopted for training the task, it being the primary choice for these kind of problems in the computer vision field in the last decade. We use the widely-used Caffe framework~\cite{jia2014} to implement the different networks. The Caffe framework contains a large set of standard layers, like convolution, max-pooling, rectified linear unit (ReLu), batch normalization as well as several more infrequent ones, and contains all the logic to pass the blobs of data between those layers. The framework allows for writing custom layers in Python to access all the blobs and outputs produced by the network.

Caffe uses configuration files based on Google Protocol buffers to define networks. To facilitate the different experiments in Chapter \ref{ch:experimentsandresults} a generator was written to produce different types of networks, combining one or multiple instances (for both image and lidar data) of a variety of backbones, with a processing network. To read in images a fetcher API running in a separate process was improved from an earlier implementation by Misra et al.~\cite{misra2016}. Running in a separate process allows for preloading the images, to reduce the time spent in the data loading layer. Initially the preprocessing to down-scaled network as explained in the previous section was executed on-the-fly during loading, but it was discovered that this significantly impacted the training time and that even a separate loading thread could not keep up with this, therefore it was chosen to pregenerate the images in the correct form.

A customized input layer was implemented in Caffe, capable of loading a sequence of images of customizable length and with a selectable batch size, to faciliate the special requirements for the inspected ordering task. This layers load the preprocessed images and adds random jittering to every pixel to help preventing the network from over-fitting on the data. Scaling and concatenation layers are used to properly feed the images into the siamese network and forward it to the appropriate parts of the processing network later. The layer also has the possibilities to generate saliency maps, a interesting visualization method proposed by Simonyan et al.~\cite{simonyan2013}. It computes the magnitude of the first derivative of the output with respect to all the pixels in the image to find the pixels which have to be changed the least to maximally affect the class score. They show that these pixels correspond to the location of the object in object detection, as can be inspected. It can also be reasoned that these saliency maps should give an indication what object on the image the network primarily used to learn.

To further investigate the contents of the network, a variety of other visualization techniques have been implemented. A basic visualization layer was implemented to show the sequences with their output including probability, which was mostly used for debugging purposes. Furthermore, a simple visualizer of the weights of the various convolutional layers has been realized. Because a convolutional filter reacts strong on a shape which is similar to its own, it is to be expected that the first layer should contain a variety of edge and corner detectors. The later layers in a network are harder to interpret and visualize, but noisy patterns can be an indication for networks which have not been trained long enough or significant overfitting due to a low regularization strength\needref. Another interesting approach is visualizing the strength of the activations, which are more difficult to interpret but should generally lead to sparser and more local maps in later layers. Finally a procedure by Girshick et al.~\cite{girshick2014} was followed for visualizing the regions in the image that show maximal activation in different layers, which is primarily interesting to find the important part of the images in the deeper convolutional layers.

All the code built for this thesis can be found online on \needref. Note that the research work is only sparsely documented and small changes are required to allow running the software stack, however everything to reproduce the results in this work is present.
