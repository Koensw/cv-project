%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Experiments and Results}
\label{ch:experimentsandresults}
In this chapter several experiments are implemented for different network design possibilities, presenting accuracy results on test data. In the context of deep learning a study of those variations with different complexity is often referred as an ablation analysis, thus this Chapter mostly presents an ablation analysis on the investigated ordering task. The required network for thetask analyzed in this thesis is split up in a backbone network and a processing network, to eventually allow for transfer learning although this is not directly attempted in this work.

For all experiments an stochastic gradient descent method with a momentum of 0.9 was used together with L2 regularization with a weight decay of 0.001. The base learning rate has been set to 0.01 with a multistep policy to decrease the learning rate by a factor 10 on the 2000, 5000 and 8000 epoch. Training was done for a total of 10000 epochs with a batch size of 16, longer iterations have not shown any significant improvements. It is stressed that finding the precise optimal value of these before mentioned parameters is not the focus of this work and therefore not done separately for all the network variations, but the values have been manually tuned for good general behavior in training. Enforcing the same values for all the different experiments made it easier to compare the results. Networks took up to around 24 hours to complete training, but it should be noted that no early stopping was enforced and most networks did not see significant improvements after approximately 6-12 hours of training.

\section{Feature maps}
A simpler version of the proposed OPN network~\cite{lee2017} has been implemented first, referred with OPN3 from now on. Here the studied permutation task was reduced to the binary ordering task and also the number of input images has been set to 3, to basically have a simple 3-way tuple task as suggested by ~\cite{misra2016} however employing the pairwise feature extraction layers and better initialization parameters for the underlying Caffenet~\cite{jia2014}. Running the suggested network from~\cite{misra2016} without those improvements was also attempted, but poor convergence has been observed. 

This OPN3 network was used with a Caffenet backbone on both the grayscale images generated with the channel splitting, color and unprocessed lidar data (from the direct projection). The results are shown in the upper part of Table \ref{tab:indiv_results}. It is immediately clear that the networks performs remarkably well on the binary classification task, with percentages above 75\% on all different training variations. This is significantly higher than the 50\% which would result from random choice and seems relatively high given the apparent complexity of the task. 

%More details on how this high percentage is reached will be studied in Chapter \ref{ch:discussion}, where an in-depth analysis of the features the network learns is presented, to try to develop a deeper understanding of the network. 

In general it can be concluded that all the studied OPN3 networks exhibit significant over-fitting, with accuracies reaching close to 100\% on the training data with proper L2 regularization employed. Despite the overfitting, accuracy percentages remain high on the test task a feature seen more often in neural networks. In any case, it becomes clear that the binary ordering task can be relatively easy for the neural network to learn, as was noticed earlier by Fernando et al.~\cite{fernando2017}. Therefore all succesful experiments on the OPN3 have also been carried out on the full OPN network\cite{lee2017}, with 4 images as input and permutation labeling. In contrast to the OPN network in the paper, which does not distinguish between forward and backward orderings of the same permutation, in this work the network are trained with all 24 permutation labels differently, as it is expected that in driving the difference between going forward (as would happen in reality) and going backward should be detectable. This variation is referred as OPN4 in the rest of this thesis.

Again the remainder of Table \ref{tab:indiv_results} shows relative strong performance of the network, despite the supposed complexity. With a fully random network gaining an accuracy of around 4\%, the version using only grayscale already shows a very significant improvement with over 48\% accuracy. On the OPN3 network it is observed that both the version on grayscale and color images work better than the unprocessed lidar which could be expected because the image generally provides a richer representation of the world with more attention to details. This behavior is also seen in the full OPN4 network with the grayscale and color images showing significantly higher accuracies compared to the unprocessed lidar. Moreover  the grayscale version performs slightly better then the color, confirming that extra regularization in the form of channel splitting helps in preventing overfitting and thus increases the overall quality of the learned features.

%supposedly because it contains enough information to develop and understanding of the world and the channel-splitting also serves as a protection against overfitting. Just like in the OPN3 network the unprocessed lidar depth is less strong than both the color and grayscale versions.

%In comparison to the OPN3 network it is speculated this is the result of the channel splitting producing too strong modifications which work good agains the overfitting in OPN3, but the complexity of the task itself already produces enough guidance in OPN4 for the network to learn better features. 

Besides the unprocessed lidar depth from the direct projection, also other data generated from the lidar is examined. First a network is trained using the lidar reflectances, which could be seen as a poor man's version of a camera. This network does surprisingly not converge at all, with accuracy sticking around 4\%, from which it follows that the added detail in grayscale and color images leads to significant improvement in the learning of features. Secondly, a network with the height above ground was investigated reaching a percentage around 35\%, higher than the unprocessed lidar depth, which might be expected to be a stronger feature. It is however interesting to note that the height above ground functions as a strong feature invariant to movement, as the height of for example a certain car remains the same in different frames independent of the movement of the particular car. Independent of this interesting proprety, it becomes clear that the depth is still a strong feature, and it becomes much stronger after applying a interpolation strategy showing very strong performance close to 60\% accuracy. In theory the neural network should be able to learn this conversions to a sparse map itself, but it is evident that additional preprocessing benefits the learning significantly. Unfortunately this suggested interpolation strategy does not naturally extends to the height above ground as this height is expected to change linearly over commonly seen objects like cars instead of mostly being constant. 

%The generation of these additional features is based on the HHA encoding\cite{gupta2013}, with the horizontal disparity map generated using interpolation of the unprocessed depth map and a height map by estimating the direction of gravity and calculating the distance above the lowest point (the measured angle is not investigated). Both those features show improved performance, with the interpolated depth in particular showing very strong performance close to 60\% accuracy, from which it can be concluded that the learning can benefit substantially from using interpolated version. It is also surprising to see that the estimated height shows relative strong performance, in particular compared to the unprocessed lidar depth. Finally the reflectances are apparently to weak source of information for the network to converge.

%%% ADD SOME REAOSING ABOUT THE LIDAR HEIGHT (AND DEPTH) HERE

\begin{table}[]
\centering
\caption{Accuracy results for the sorting task on different individual features (italic indicates non-converging)}
\label{tab:indiv_results}
\begin{tabular}{|p{7.5cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Input}                                                          & \textbf{Output network} & \textit{\textbf{Accuracy}} \\ \hline
grayscale                                                               & OPN3                    & 82.25 \%                   \\ \hline
color                                                                   & OPN3                    & 84.25 \%                   \\ \hline
unprocessed lidar depth                                                 & OPN3                    & 75.93 \%                   \\ \hline
grayscale                                                               & OPN4                    & 51.16 \%                   \\ \hline
color                                                                   & OPN4                    & 44.48 \%                   \\ \hline
unprocessed lidar depth                                                 & OPN4                    & 27.25 \%                   \\ \hline
unprocessed lidar reflectances                                          & OPN4                    & \textit{4.76} \%                   \\ \hline
lidar height                                                            & OPN4                    & 35.17 \%                   \\ \hline
interpolated lidar depth                                                & OPN4                    & 59.28 \%                   \\ \hline
\end{tabular}
\end{table}

\section{Combining features}
Besides viewing the features as single information source, it could be hypothesized that the lidar provides additional information to enrich the representation learned from camera alone and combinations of the feature maps should show even stronger performs. To study this in more detail several networks have been implemented containing separate backbones for both the lidar and image data for the initial layers. These backbones combine at different depths of the network to a single backbone processing the rest of the convolutional layers as single features. The concatenation of neurons is always applied after the possible pooling, batch norm and ReLU steps at a particular level, precisely before the next convolutional layer begins.

Initial experiments were carried out on the OPN3 network with the unprocessed lidar and grayscale images to get an general understanding of merging on different depths - better combinations of features for acquiring maximum accuracy are investigated next. Table \ref{tab:merge_results} shows that merging after the first convolutional layer gives indeed an improvement, but the difference is very marginal with 0.4\% over using only grayscale, and this can definitely fall within the limit of uncertainty of the training. It can be concluded that on the OPN3 task the enriched data does not lead to improved performance. 

Further investigations were performed on the OPN4 network to find the strength of different feature combinations. The three strongest feature maps on OPN4 with grayscale images, lidar height and interpolated lidar depth were selected for the more detailed investigation (color was not selected although it being stronger than lidar height, because both are generated from the same data source). While this combination of features show much stronger performance, it is rather surprisingly not better than the invidual features learned from scratch, from which it appears that combining the features does not lead to improved performance. 

Looking at the influence of the merging depth, it becomes clear that merging further in the network tend to be weaker and it can even be seen that merging after the fourth convolutional layer does not lead to convergence at all. Late fusion has however earlier been found to perform better\cite{schlosser2016}, although significant cost is noted for the large increase in the number of parameters. It is hypothesized that the difficulty of the learning problem increases so substantially in this task, with more parameters added for the various input sources, that the lack of training data could be a reason that no improved performance is seen on this particular task. 
%[OPN4 is still missing...]
%Further investigations were performed on the OPN4 network to find the strength of different feature combinations. The combination of grayscale and lidar shows much stronger performance on the OPN4 network, with a difference of around 6\%, considerably higher than the improvement in the OPN3 network.

\begin{table}[]
\centering
\caption{Accuracy results on the trained networks (italic indicates non-converging)}
\label{tab:merge_results}
\begin{tabular}{|p{7.5cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Input}                                                          & \textbf{Output network} & \textit{\textbf{Accuracy}} \\ \hline
grayscale and unprocessed lidar merged after first convolution          & OPN3                    & 82.84 \%                   \\ \hline
grayscale and unprocessed lidar merged after third convolution          & OPN3                    & 79.48 \%                   \\ \hline
grayscale and unprocessed lidar merged after fourth convolution         & OPN3                    & \textit{49.60} \%          \\ \hline
grayscale, interpolated lidar depth and lidar height merged after first convolution          & OPN4                    & 56.30 (?) \%                   \\ \hline
grayscale, interpolated lidar depth and lidar height merged after second convolution         & OPN4                    & 59.18 \%                   \\ \hline
grayscale, interpolated lidar depth and lidar height merged after third convolution          & OPN4                    & 52.70 \%                   \\ \hline
grayscale, interpolated lidar depth and lidar height merged after fourth convolution         & OPN4                    & \textit{4.29} \%                   \\ \hline
\end{tabular}
\end{table}

\section{Additional experiments}
As earlier works~\cite{misra2016,lee2017} have used the Caffenet backbone, the networks considered here so far, are focused on this architecture as well. However several other networks have shown superior performance on object detection on the Imagenet dataset, notably the Resnet architecture\cite{he2016}. The Resnet architecture has versions with various depths with increasing strength on particular task if properly trained. However because the binary ordering task has simple output and to shorten the learning time it could be argued that Resnet-18 should be solid choice to start. Testing on the OPN3 datasets and keeping other parameters similar to earlier experiments results the network however fails to converge with an accuracy of 53.12\% only slightly better than random and far worse then the networks based on Caffenet. The exact reason for this behavior has not been investigated, but it can be concluded that the Resnet architecture does apparently not easily adapt to this problem.

In another experiment the influence of the constant timing offset between the different frames was investigated on the OPN3 network. Under the assumption of relative constant velocity the travelled distances between frame with the same time offset is frequently on a similar scale. To check the influence of this a more difficult task was run with the time delay of all training data changed from the fixed 4 or 5 frames, to a sampled one from 4 until 10 frames. The eventual network was evaluated on the same test set as all other OPN3 candidates. An accuracy of 79.72\% was reached, from which it is clear that the learning does indeed exploits the constant offsets, but the impact of this effect is rather small.

% In general all the networks that converge exhibit significant over-fitting, with accuracies reaching close to 100\% on the training data with the L2 regularization employed. As accuracy percentages are also high on the test task it is clear that the task is relatively easy for the neural network to learn, as was noticed earlier by Fernando et al.~\cite{fernando2017}. Therefore all succesful experiments on the OPN3 have also been carried out on the full OPN network\cite{lee2017}, with 4 images as input and permutation labeling. In contrast to the OPN4 network which does not distinguish between forward and backward orderings of the same permutation, in this work the network are trained with all permutation labels differently as in driving the difference between going forward (as would happen in reality) and going backward should generally be detectable. 

% grayscale with training on non-fixed time differences                   & Caffenet                  & OPN3                    & 79.72 \%                   \\ \hline
% grayscale                                                               & Resnet-18                 & OPN3                    & \textit{53.12} \%          \\ \hline
% grayscale                                                               & Caffenet                  & OPN4                    & 38.63 \%                   \\ \hline
% grayscale and unprocessed lidar merged after first convolution          & Caffenet                  & OPN4                    & 44.42 \%                   \\ \hline

%In an initial experiment, we implement the tuple network from\cite{misra2016}. [However it did not converge].

%Describe the evaluation you did in a way, such that an independent researcher can repeat it. Cover the following questions:
% \begin{itemize}
%  \item \textit{What is the experimental setup and methodology?} Describe the setting of the experiments and give all the parameters in detail which you have used. Give a detailed account of how the experiment was conducted.
%  \item \textit{What are your results?} In this section, a \emph{clear description} of the results is given. If you produced lots of data, include only representative data here and put all results into the appendix. 
% \end{itemize}
