%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Experiments and Results}
\label{ch:experimentsandresults}
In this chapter several experiments are worked out for different network design possibilities, presenting accuracy results on test data. In the context of deep learning a study of those variations with different complexity is often referred as an ablation analysis. The required network for the ordering task analyzed in this thesis is split up in a backbone network and a processing network to generate the eventual output. 

For all experiments an stochastic gradient descent method with a momentum of 0.9 was used together with L2 regularization with a weight decay of 0.001. The base learning rate has been set to 0.01 with a multistep policy to decrease the learning rate by a factor 10 on the 2000, 5000 and 8000 epoch. Training was done for a total of 10000 epochs with a batch size of 16, longer iterations have not shown any significant improvements. It is stressed that finding the precise optimal value of these before mentioned parameters is not the focus of this work and therefore not done separately for all the network variations, but the values have been manually tuned for good general behavior in training. Enforcing the same values for all the different experiments made it easier to compare the results. Networks took up to around 24 hours to complete training, but it should be noted that no early stopping was enforced and most networks did not see significant improvements after approximately 6-12 hours of training.

\section{Feature maps}
A simpler version of the proposed OPN network~\cite{lee2017} has been implemented first, referred with OPN3 from now on. Here the studied permutation task was reduced to the binary ordering task and also the number of input images has been set to 3, to basically have a simple 3-way tuple task as suggested by ~\cite{misra2016} however employing the pairwise feature extraction layers and better initialization parameters for the underlying Caffenet~\cite{jia2014}. Running the suggested network from~\cite{misra2016} without those improvements was also attempted, but poor convergence has been observed. 

This OPN3 network was used with a Caffenet backbone on both the grayscale images generated with the channel splitting, color and unprocessed lidar data (from the direct projection). The results are shown in the upper part of Table \ref{tab:indiv_results}. It is immediately clear that the networks performs remarkably well on the binary classification task, with percentages above 75\% on all different training variations. This is significantly higher than the 50\% which would result from random choice and seems relatively high given the apparent complexity of the task. How this high percentage is reached will be studied in more detail in the Chapter \ref{ch:discussion}, where an in-depth analysis of the features the network learns is presented, to try to develop a deeper understanding of the network. 

In general it could however be concluded that all the OPN3 networks that converge exhibit significant over-fitting, with accuracies reaching close to 100\% on the training data with proper L2 regularization employed. Despite the overfitting, accuracy percentages are also high on the test task a feature seen more often in neural networks. In any case, it becomes clear that the binary ordering task can be relatively easy for the neural network to learn, as was noticed earlier by Fernando et al.~\cite{fernando2017}. Therefore all succesful experiments on the OPN3 have also been carried out on the full OPN network\cite{lee2017}, with 4 images as input and permutation labeling. In contrast to the OPN network in the paper, which does not distinguish between forward and backward orderings of the same permutation, in this work the network are trained with all 24 permutation labels differently, as it is expected that in driving the difference between going forward (as would happen in reality) and going backward should be detectable. This variation is referred as OPN4 in the rest of this thesis.

Again the remainder of Table \ref{tab:indiv_results} shows relative strong performance of the network, despite the supposed complexity. With a fully random network gaining an accuracy of around 4\%, the version using only grayscale already shows significant improvement with over 38\%. On the OPN3 network it is observed that both the version on grayscale and color images work better than the unprocessed lidar which could be expected because the image generally provides a richer representation of the world with more attention to details. However this behavior is not seen in the full OPN4 network where the color still performs better, but the grayscale version (produced using channel splitting) performs relatively worse. In comparison to the OPN3 network it is speculated this is the result of the channel splitting producing too strong modifications which work good agains the overfitting in OPN3, but the complexity of the task itself already produces enough guidance in OPN4 for the network to learn better features. 

Besides the unprocessed lidar depth from the direct projection, also other data generated from the lidar is examined. In theory the neural network should be able to learn those  
conversions itself, but it expected that additional preprocessing would benefit the learning significantly. The generation of these additional features is based on the HHA encoding\cite{gupta2013}, with the horizontal disparity map generated using interpolation [METHOD?] of the unprocessed depth map and a height map by estimating the direction of gravity and calculating the distance above the lowest point (the measured angle is not investigated). Both those features show improved performance, with the interpolated depth interestingly producing almost similar performance as using a color camera. More surprisingly the estimated height shows strong performance, which implies the height above ground is a good information source.
\begin{table}[]
\centering
\caption{Accuracy results for the sorting task on different individual features}
\label{tab:indiv_results}
\begin{tabular}{|p{7cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Input}                                                          & \textbf{Output network} & \textit{\textbf{Accuracy}} \\ \hline
grayscale                                                               & OPN3                    & 82.25 \%                   \\ \hline
color                                                                   & OPN3                    & 84.25 \%                   \\ \hline
unprocessed lidar depth                                                 & OPN3                    & 75.93 \%                   \\ \hline
grayscale                                                               & OPN4                    & 38.63 \%                   \\ \hline
color                                                                   & OPN4                    & 48.49 \%                   \\ \hline
unprocessed lidar depth                                                 & OPN4                    & 27.25 \%                   \\ \hline
unprocessed lidar reflectances                                          & OPN4                    & 4.76 \%                   \\ \hline
interpolated lidar depth                                                & OPN4                    & 59.28 \%                   \\ \hline
lidar height                                                            & OPN4                    & 35.17 \%                   \\ \hline
\end{tabular}
\end{table}

\section{Combining features}
Besides viewing the features as single information source, it should be expected that the lidar provides additional information to enrich the representation learned from camera alone. To study this in more detail several networks have been implemented containing separate backbones for both the lidar and image data for the initial layers. These backbones combine at different depths of the network to a single backbone. This concatenation of neurons is always applied after the possible pooling, batch norm and ReLU steps at a particular depth, precisely before the next convolutional layer.

Initial experiments where carried out on the OPN3 network with the unprocessed lidar and grayscale images to get an general understanding of merging on different depths - better combinations of features for acquiring maximum accuracy are investigated later on. Table \ref{tab:merge_results} shows that merging after the first convolutional layer gives indeed an improvement, but the difference is very marginal with 0.4\% over using only grayscale, and could well fall within the limit of uncertainty of the training. Merging further in the chain is more expensive as the number of parameters to learn increases. It is established that the difficulty of the learning problem increases so substantially, that the accuracy with combined features does apparently not give any improved performance and it can even be seen that merging after the fourth convolutional layer does not lead to convergence at all. 

Further investigations were performed on the OPN4 network to find the strength of different feature combinations. The combination of grayscale and lidar shows much stronger performance on the OPN4 network, with a difference of around 6\%, considerably higher than the improvement in the OPN3 network.

\begin{table}[]
\centering
\caption{Accuracy results on the trained networks}
\label{tab:merge_results}
\begin{tabular}{|p{7cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Input}                                                          & \textbf{Output network} & \textit{\textbf{Accuracy}} \\ \hline
grayscale and unprocessed lidar merged after first convolution          & OPN3                    & 82.84 \%                   \\ \hline
grayscale and unprocessed lidar merged after third convolution          & OPN3                    & 79.48 \%                   \\ \hline
grayscale and unprocessed lidar merged after fourth convolution         & OPN3                    & \textit{49.25} \%          \\ \hline
grayscale and unprocessed lidar merged after first convolution          & OPN4                    & 50.49 \%                   \\ \hline
grayscale and unprocessed lidar merged after second convolution         & OPN4                    & - \%                       \\ \hline
grayscale and unprocessed lidar merged after third convolution          & OPN3                    & 49.49 \%                   \\ \hline
\end{tabular}
\end{table}

\section{Additional experiments}
As earlier works~\cite{misra2016,lee2017} have used the Caffenet backbone, the original networks here are focused on those as well. However several other networks have shown superior performance on object detection on the Imagenet dataset, notably the Resnet architecture\cite{he2016}. The Resnet architecture has versions with various depths with increasing strength on particular task if properly trained. However because the binary ordering task has simple output and to shorten the learning time it could be argued that Resnet-18 should be solid choice to start. Testing on the OPN3 datasets and keeping other parameters similar to earlier experiments results the network however fails to converge with an accuracy of 53.12\% only slightly better than random and far worse then the networks based on Caffenet. The exact reason for this behavior has not been investigated, but it can be concluded that the Resnet architecture does not easily adapt to this problem.

In another experiment the influence of the constant timing offset between the different frames was investigated. Under the assumption of relative constant velocity the travelled distances between frame with the same time offset is frequently on a similar scale. To check the influence of this a more difficult task was run with the time delay of all training data changed from the fixed 5 frames, to a sampled one from 4 until 10 frames. The eventual network was evaluated on the same test set as all other OPN3 candidates. An accuracy of 79.72\% was reached, from which it is clear that the learning does indeed make use of the constant offsets, but the impact of this effect is rather small.

% In general all the networks that converge exhibit significant over-fitting, with accuracies reaching close to 100\% on the training data with the L2 regularization employed. As accuracy percentages are also high on the test task it is clear that the task is relatively easy for the neural network to learn, as was noticed earlier by Fernando et al.~\cite{fernando2017}. Therefore all succesful experiments on the OPN3 have also been carried out on the full OPN network\cite{lee2017}, with 4 images as input and permutation labeling. In contrast to the OPN4 network which does not distinguish between forward and backward orderings of the same permutation, in this work the network are trained with all permutation labels differently as in driving the difference between going forward (as would happen in reality) and going backward should generally be detectable. 

% grayscale with training on non-fixed time differences                   & Caffenet                  & OPN3                    & 79.72 \%                   \\ \hline
% grayscale                                                               & Resnet-18                 & OPN3                    & \textit{53.12} \%          \\ \hline
% grayscale                                                               & Caffenet                  & OPN4                    & 38.63 \%                   \\ \hline
% grayscale and unprocessed lidar merged after first convolution          & Caffenet                  & OPN4                    & 44.42 \%                   \\ \hline

%In an initial experiment, we implement the tuple network from\cite{misra2016}. [However it did not converge].

%Describe the evaluation you did in a way, such that an independent researcher can repeat it. Cover the following questions:
% \begin{itemize}
%  \item \textit{What is the experimental setup and methodology?} Describe the setting of the experiments and give all the parameters in detail which you have used. Give a detailed account of how the experiment was conducted.
%  \item \textit{What are your results?} In this section, a \emph{clear description} of the results is given. If you produced lots of data, include only representative data here and put all results into the appendix. 
% \end{itemize}
